"""
Analisador Tem√°tico para Separa√ß√£o de Conte√∫do por Temas
"""
import logging
import re
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from collections import Counter
import json
from datetime import datetime

# Text processing
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import RSLPStemmer
import spacy

# ML for topic classification
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import LatentDirichletAllocation
import numpy as np

from config import *

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ThematicAnalyzer:
    """Analisador tem√°tico para classifica√ß√£o e separa√ß√£o de conte√∫do"""
    
    def __init__(self):
        self.setup_nltk()
        self.setup_spacy()
        self.stemmer = RSLPStemmer()
        self.stop_words = set(stopwords.words('portuguese'))
        
        # Temas predefinidos para classifica√ß√£o
        self.predefined_themes = {
            'inteligencia_artificial': {
                'keywords': ['ia', 'intelig√™ncia artificial', 'machine learning', 'deep learning', 
                           'neural network', 'algoritmo', 'modelo', 'treinamento', 'dados'],
                'description': 'Intelig√™ncia Artificial e Machine Learning'
            },
            'programacao': {
                'keywords': ['programa√ß√£o', 'c√≥digo', 'python', 'javascript', 'java', 'algoritmo',
                           'fun√ß√£o', 'vari√°vel', 'loop', 'condi√ß√£o', 'debugging'],
                'description': 'Programa√ß√£o e Desenvolvimento de Software'
            },
            'matematica': {
                'keywords': ['matem√°tica', 'c√°lculo', '√°lgebra', 'geometria', 'estat√≠stica',
                           'probabilidade', 'derivada', 'integral', 'equa√ß√£o', 'fun√ß√£o'],
                'description': 'Matem√°tica e Estat√≠stica'
            },
            'fisica': {
                'keywords': ['f√≠sica', 'mec√¢nica', 'termodin√¢mica', 'eletromagnetismo', '√≥ptica',
                           'energia', 'for√ßa', 'velocidade', 'acelera√ß√£o', 'onda'],
                'description': 'F√≠sica e Ci√™ncias Naturais'
            },
            'quimica': {
                'keywords': ['qu√≠mica', 'mol√©cula', '√°tomo', 'rea√ß√£o', 'composto', 'elemento',
                           'tabela peri√≥dica', 'liga√ß√£o', 'solu√ß√£o', '√°cido', 'base'],
                'description': 'Qu√≠mica e Ci√™ncias Qu√≠micas'
            },
            'biologia': {
                'keywords': ['biologia', 'c√©lula', 'DNA', 'prote√≠na', 'organismo', 'evolu√ß√£o',
                           'gen√©tica', 'ecossistema', 'biodiversidade', 'anatomia'],
                'description': 'Biologia e Ci√™ncias Biol√≥gicas'
            },
            'historia': {
                'keywords': ['hist√≥ria', 'hist√≥rico', 'passado', 'antigo', 'medieval', 'moderno',
                           'guerra', 'revolu√ß√£o', 'civiliza√ß√£o', 'cultura', 'sociedade'],
                'description': 'Hist√≥ria e Ci√™ncias Humanas'
            },
            'literatura': {
                'keywords': ['literatura', 'livro', 'poesia', 'romance', 'autor', 'escritor',
                           'narrativa', 'personagem', 'enredo', 'estilo', 'linguagem'],
                'description': 'Literatura e L√≠ngua Portuguesa'
            },
            'economia': {
                'keywords': ['economia', 'financeiro', 'mercado', 'capital', 'investimento',
                           'infla√ß√£o', 'PIB', 'moeda', 'banco', 'cr√©dito'],
                'description': 'Economia e Finan√ßas'
            },
            'filosofia': {
                'keywords': ['filosofia', 'filos√≥fico', '√©tica', 'moral', 'l√≥gica', 'raz√£o',
                           'conhecimento', 'verdade', 'exist√™ncia', 'pensamento'],
                'description': 'Filosofia e √âtica'
            }
        }
    
    def setup_nltk(self):
        """Configura recursos do NLTK"""
        try:
            nltk.download('punkt', quiet=True)
            nltk.download('stopwords', quiet=True)
            nltk.download('rslp', quiet=True)
        except Exception as e:
            logger.warning(f"Erro ao configurar NLTK: {e}")
    
    def setup_spacy(self):
        """Configura spaCy para portugu√™s"""
        try:
            self.nlp = spacy.load('pt_core_news_sm')
        except OSError:
            logger.warning("Modelo spaCy portugu√™s n√£o encontrado. Usando processamento b√°sico.")
            self.nlp = None
    
    def extract_keywords(self, text: str, max_keywords: int = 20) -> List[str]:
        """Extrai palavras-chave do texto"""
        try:
            # Tokeniza√ß√£o e limpeza
            words = word_tokenize(text.lower())
            words = [word for word in words if word.isalpha() and len(word) > 2]
            words = [word for word in words if word not in self.stop_words]
            
            # Stemming
            stemmed_words = [self.stemmer.stem(word) for word in words]
            
            # Contagem de frequ√™ncia
            word_freq = Counter(stemmed_words)
            
            # Retorna as palavras mais frequentes
            return [word for word, freq in word_freq.most_common(max_keywords)]
            
        except Exception as e:
            logger.error(f"Erro ao extrair palavras-chave: {e}")
            return []
    
    def classify_theme(self, text: str) -> Tuple[str, float]:
        """Classifica o tema do texto"""
        try:
            # Extrai palavras-chave do texto
            keywords = self.extract_keywords(text)
            
            # Calcula similaridade com temas predefinidos
            theme_scores = {}
            
            for theme_name, theme_info in self.predefined_themes.items():
                score = 0
                theme_keywords = [kw.lower() for kw in theme_info['keywords']]
                
                # Conta correspond√™ncias de palavras-chave
                for keyword in keywords:
                    for theme_keyword in theme_keywords:
                        if keyword in theme_keyword or theme_keyword in keyword:
                            score += 1
                
                # Normaliza o score
                if len(keywords) > 0:
                    theme_scores[theme_name] = score / len(keywords)
                else:
                    theme_scores[theme_name] = 0
            
            # Retorna o tema com maior score
            if theme_scores:
                best_theme = max(theme_scores, key=theme_scores.get)
                confidence = theme_scores[best_theme]
                
                # Se a confian√ßa for muito baixa, classifica como 'geral'
                if confidence < 0.1:
                    return 'geral', confidence
                else:
                    return best_theme, confidence
            else:
                return 'geral', 0.0
                
        except Exception as e:
            logger.error(f"Erro ao classificar tema: {e}")
            return 'geral', 0.0
    
    def analyze_document_themes(self, documents: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """Analisa temas dos documentos e os agrupa"""
        try:
            thematic_groups = {}
            
            for doc in documents:
                content = doc.get('content', {})
                text = content.get('text', '')
                
                if not text:
                    continue
                
                # Classifica o tema
                theme, confidence = self.classify_theme(text)
                
                # Adiciona informa√ß√µes do tema ao documento
                doc['theme'] = theme
                doc['theme_confidence'] = confidence
                doc['theme_description'] = self.predefined_themes.get(theme, {}).get('description', 'Tema Geral')
                
                # Agrupa por tema
                if theme not in thematic_groups:
                    thematic_groups[theme] = []
                
                thematic_groups[theme].append(doc)
            
            return thematic_groups
            
        except Exception as e:
            logger.error(f"Erro ao analisar temas dos documentos: {e}")
            return {}
    
    def generate_thematic_summary(self, thematic_groups: Dict[str, List[Dict[str, Any]]]) -> str:
        """Gera resumo tem√°tico dos grupos"""
        try:
            summary_parts = []
            
            summary_parts.append("# üìö An√°lise Tem√°tica dos Documentos")
            summary_parts.append(f"**Gerado em:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
            summary_parts.append("")
            
            # Estat√≠sticas gerais
            total_docs = sum(len(docs) for docs in thematic_groups.values())
            total_themes = len(thematic_groups)
            
            summary_parts.append(f"## üìä Estat√≠sticas Gerais")
            summary_parts.append(f"- **Total de documentos:** {total_docs}")
            summary_parts.append(f"- **Temas identificados:** {total_themes}")
            summary_parts.append("")
            
            # An√°lise por tema
            summary_parts.append(f"## üéØ An√°lise por Tema")
            summary_parts.append("")
            
            for theme, docs in thematic_groups.items():
                theme_info = self.predefined_themes.get(theme, {})
                theme_description = theme_info.get('description', 'Tema Geral')
                
                summary_parts.append(f"### {theme_description}")
                summary_parts.append(f"- **Tema:** {theme}")
                summary_parts.append(f"- **Documentos:** {len(docs)}")
                
                # Lista documentos do tema
                for i, doc in enumerate(docs, 1):
                    file_path = Path(doc.get('file_path', '')).name
                    confidence = doc.get('theme_confidence', 0)
                    summary_parts.append(f"  {i}. {file_path} (confian√ßa: {confidence:.2f})")
                
                summary_parts.append("")
            
            # Recomenda√ß√µes
            summary_parts.append(f"## üí° Recomenda√ß√µes")
            summary_parts.append("")
            summary_parts.append(f"- **Temas mais frequentes:** {self._get_most_frequent_themes(thematic_groups)}")
            summary_parts.append(f"- **Diversidade tem√°tica:** {'Alta' if total_themes > 5 else 'M√©dia' if total_themes > 2 else 'Baixa'}")
            summary_parts.append(f"- **Pr√≥ximos passos:** Gerar resumos e audiobooks por tema")
            summary_parts.append("")
            
            summary_parts.append("---")
            summary_parts.append("*An√°lise tem√°tica gerada automaticamente pelo Sistema RAG Local*")
            
            return "\n".join(summary_parts)
            
        except Exception as e:
            logger.error(f"Erro ao gerar resumo tem√°tico: {e}")
            return f"Erro ao gerar resumo tem√°tico: {str(e)}"
    
    def _get_most_frequent_themes(self, thematic_groups: Dict[str, List[Dict[str, Any]]]) -> str:
        """Retorna os temas mais frequentes"""
        try:
            theme_counts = {theme: len(docs) for theme, docs in thematic_groups.items()}
            sorted_themes = sorted(theme_counts.items(), key=lambda x: x[1], reverse=True)
            
            if len(sorted_themes) >= 2:
                return f"{sorted_themes[0][0]} ({sorted_themes[0][1]} docs), {sorted_themes[1][0]} ({sorted_themes[1][1]} docs)"
            elif len(sorted_themes) == 1:
                return f"{sorted_themes[0][0]} ({sorted_themes[0][1]} docs)"
            else:
                return "Nenhum tema identificado"
                
        except Exception as e:
            logger.error(f"Erro ao calcular temas frequentes: {e}")
            return "Erro na an√°lise"
    
    def create_thematic_structure(self, thematic_groups: Dict[str, List[Dict[str, Any]]]) -> Dict[str, str]:
        """Cria estrutura de diret√≥rios por tema"""
        try:
            thematic_dirs = {}
            
            for theme, docs in thematic_groups.items():
                # Cria diret√≥rio do tema
                theme_dir = RAGFILES_DIR / f"temas/{theme}"
                theme_dir.mkdir(parents=True, exist_ok=True)
                
                thematic_dirs[theme] = str(theme_dir)
                
                # Cria subdiret√≥rios
                (theme_dir / "resumos").mkdir(exist_ok=True)
                (theme_dir / "audiobooks").mkdir(exist_ok=True)
                (theme_dir / "dados").mkdir(exist_ok=True)
            
            return thematic_dirs
            
        except Exception as e:
            logger.error(f"Erro ao criar estrutura tem√°tica: {e}")
            return {}
    
    def get_processor_info(self) -> Dict[str, Any]:
        """Retorna informa√ß√µes sobre o analisador tem√°tico"""
        return {
            'name': 'ThematicAnalyzer',
            'version': '1.0.0',
            'description': 'Analisador tem√°tico para classifica√ß√£o e separa√ß√£o de conte√∫do',
            'supported_themes': list(self.predefined_themes.keys()),
            'features': [
                'Classifica√ß√£o autom√°tica de temas',
                'Agrupamento de documentos por tema',
                'Gera√ß√£o de resumos tem√°ticos',
                'Cria√ß√£o de estrutura de diret√≥rios',
                'An√°lise de confian√ßa tem√°tica'
            ]
        }
