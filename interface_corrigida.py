#!/usr/bin/env python3
"""
Interface Web Corrigida - Sistema RAG Local
Interface com configuraÃ§Ã£o de diretÃ³rio, listagem de arquivos e resultados Ãºteis
"""

import streamlit as st
import os
import sys
import time
import logging
from pathlib import Path
from datetime import datetime
import json
import sqlite3
import hashlib
import re
from typing import List, Dict, Any
import urllib.request
from html.parser import HTMLParser

# Adicionar o diretÃ³rio atual ao path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ConfiguraÃ§Ã£o da pÃ¡gina
st.set_page_config(
    page_title="RAG Local - Interface Corrigida",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Estado da sessÃ£o
if 'processing_status' not in st.session_state:
    st.session_state['processing_status'] = {
        'is_processing': False,
        'current_file': None,
        'processed_files': [],
        'failed_files': [],
        'total_files': 0,
        'progress': 0,
        'logs': [],
        'start_time': None
    }

if 'rag_system' not in st.session_state:
    st.session_state['rag_system'] = None

if 'vector_db_path' not in st.session_state:
    st.session_state['vector_db_path'] = "vector_db.sqlite"

if 'documents_dir' not in st.session_state:
    st.session_state['documents_dir'] = "/home/lsantann/Documents/CC/"

if 'selected_files' not in st.session_state:
    st.session_state['selected_files'] = []

# ============================================================================
# SISTEMA RAG CORRIGIDO
# ============================================================================

class RAGSystem:
    """Sistema RAG corrigido com melhor qualidade de resultados"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.init_db()
    
    def init_db(self):
        """Inicializar banco de dados"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS documents (
                id TEXT PRIMARY KEY,
                file_path TEXT,
                file_type TEXT,
                content TEXT,
                metadata TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS chunks (
                id TEXT PRIMARY KEY,
                document_id TEXT,
                chunk_text TEXT,
                chunk_index INTEGER,
                vector TEXT,
                metadata TEXT,
                FOREIGN KEY (document_id) REFERENCES documents (id)
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def create_embedding(self, text: str) -> List[float]:
        """Criar embedding melhorado"""
        # TokenizaÃ§Ã£o e limpeza
        words = re.findall(r'\b\w+\b', text.lower())
        
        # Remover stopwords
        stopwords = {
            'a', 'o', 'e', 'de', 'do', 'da', 'em', 'para', 'com', 'por', 'que', 'um', 'uma',
            'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
            'Ã©', 'sÃ£o', 'foi', 'ser', 'ter', 'estar', 'ter', 'fazer', 'poder', 'querer'
        }
        words = [w for w in words if w not in stopwords and len(w) > 2]
        
        # Calcular frequÃªncia
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # Criar vetor de 256 dimensÃµes
        vector = [0.0] * 256
        
        for word, freq in word_freq.items():
            hash_val = hash(word) % 256
            vector[hash_val] += freq
        
        # Normalizar
        norm = sum(x * x for x in vector) ** 0.5
        if norm > 0:
            vector = [x / norm for x in vector]
        
        return vector
    
    def query(self, question: str, max_results: int = 5) -> List[Dict]:
        """Consulta melhorada com resultados Ãºteis"""
        query_embedding = self.create_embedding(question)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Buscar chunks com informaÃ§Ãµes do documento
        cursor.execute("""
            SELECT c.id, c.document_id, c.chunk_text, c.vector, c.metadata, d.file_path, d.file_type
            FROM chunks c
            JOIN documents d ON c.document_id = d.id
        """)
        results = []
        
        for row in cursor.fetchall():
            chunk_id, doc_id, chunk_text, vector_json, metadata_json, file_path, file_type = row
            vector = json.loads(vector_json) if vector_json else []
            metadata = json.loads(metadata_json) if metadata_json else {}
            
            if vector:
                # Similaridade vetorial
                dot_product = sum(a * b for a, b in zip(query_embedding, vector))
                norm1 = sum(a * a for a in query_embedding) ** 0.5
                norm2 = sum(b * b for b in vector) ** 0.5
                vector_sim = dot_product / (norm1 * norm2) if norm1 > 0 and norm2 > 0 else 0
            else:
                vector_sim = 0
            
            # Similaridade textual melhorada
            question_words = set(re.findall(r'\b\w+\b', question.lower()))
            chunk_words = set(re.findall(r'\b\w+\b', chunk_text.lower()))
            
            if question_words and chunk_words:
                jaccard_sim = len(question_words.intersection(chunk_words)) / len(question_words.union(chunk_words))
            else:
                jaccard_sim = 0
            
            # Similaridade combinada
            combined_sim = (vector_sim * 0.6) + (jaccard_sim * 0.4)
            
            if combined_sim > 0.05:  # Threshold mais baixo
                results.append({
                    'chunk_id': chunk_id,
                    'document_id': doc_id,
                    'chunk_text': chunk_text,
                    'similarity': combined_sim,
                    'vector_sim': vector_sim,
                    'text_sim': jaccard_sim,
                    'metadata': metadata,
                    'file_path': file_path,
                    'file_type': file_type
                })
        
        conn.close()
        
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:max_results]
    
    def get_stats(self):
        """Obter estatÃ­sticas"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("SELECT COUNT(*) FROM documents")
        doc_count = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM chunks")
        chunk_count = cursor.fetchone()[0]
        
        cursor.execute("SELECT file_path, file_type, COUNT(*) FROM documents GROUP BY file_path, file_type")
        file_info = cursor.fetchall()
        
        conn.close()
        
        return {
            'documents': doc_count,
            'chunks': chunk_count,
            'file_info': file_info
        }
    
    def generate_summary(self, question: str, results: List[Dict]) -> str:
        """Gerar resumo em markdown dos resultados"""
        if not results:
            return "Nenhum resultado encontrado."
        
        summary = f"# Resumo da Consulta: {question}\n\n"
        summary += f"**Data:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\n\n"
        summary += f"**Total de resultados:** {len(results)}\n\n"
        
        for i, result in enumerate(results, 1):
            file_name = Path(result['file_path']).name
            summary += f"## Resultado {i}\n\n"
            summary += f"**Arquivo:** {file_name}\n"
            summary += f"**Tipo:** {result['file_type']}\n"
            summary += f"**Similaridade:** {result['similarity']:.3f}\n"
            summary += f"**Vetorial:** {result['vector_sim']:.3f} | **Textual:** {result['text_sim']:.3f}\n\n"
            summary += f"**ConteÃºdo:**\n```\n{result['chunk_text'][:500]}...\n```\n\n"
            summary += "---\n\n"
        
        return summary
    
    def save_summary(self, question: str, results: List[Dict]) -> str:
        """Salvar resumo em arquivo markdown"""
        summary = self.generate_summary(question, results)
        
        # Criar diretÃ³rio de resumos
        summaries_dir = Path("summaries")
        summaries_dir.mkdir(exist_ok=True)
        
        # Nome do arquivo baseado na pergunta
        safe_question = re.sub(r'[^\w\s-]', '', question).strip()
        safe_question = re.sub(r'[-\s]+', '-', safe_question)
        filename = f"resumo_{safe_question[:50]}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        file_path = summaries_dir / filename
        
        # Salvar arquivo
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(summary)
        
        return str(file_path)

# ============================================================================
# PROCESSAMENTO CORRIGIDO
# ============================================================================

def get_available_files(directory: str) -> List[Path]:
    """Obter arquivos disponÃ­veis no diretÃ³rio"""
    dir_path = Path(directory)
    if not dir_path.exists():
        return []
    
    # Todas as extensÃµes suportadas
    supported_extensions = [
        # Texto
        '.txt', '.md', '.rst',
        # PDF
        '.pdf',
        # Imagens
        '.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff', '.webp',
        # Documentos
        '.docx', '.doc', '.pptx', '.ppt',
        # ODF
        '.odt', '.ods', '.odp',
        # CÃ³digo
        '.py', '.js', '.ts', '.java', '.cpp', '.c', '.h', '.hpp', '.cs', '.php', '.rb', '.go', '.rs', '.swift', '.kt', '.scala', '.r', '.m', '.pl', '.sh', '.sql', '.html', '.css', '.xml', '.json', '.yaml', '.yml'
    ]
    
    all_files = []
    
    for ext in supported_extensions:
        all_files.extend(dir_path.rglob(f"*{ext}"))
    
    return all_files

def extract_content_from_file(file_path: Path) -> str:
    """Extrair conteÃºdo de diferentes tipos de arquivo"""
    try:
        file_ext = file_path.suffix.lower()
        
        # Arquivos de texto
        if file_ext in ['.txt', '.md', '.rst', '.py', '.js', '.ts', '.java', '.cpp', '.c', '.h', '.hpp', '.cs', '.php', '.rb', '.go', '.rs', '.swift', '.kt', '.scala', '.r', '.m', '.pl', '.sh', '.sql', '.html', '.css', '.xml', '.json', '.yaml', '.yml']:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                return f.read()
        
        # PDF
        elif file_ext == '.pdf':
            try:
                import PyPDF2
                with open(file_path, 'rb') as f:
                    pdf_reader = PyPDF2.PdfReader(f)
                    text = ""
                    for page in pdf_reader.pages:
                        text += page.extract_text() + "\n"
                    return text
            except ImportError:
                return f"Arquivo PDF {file_path.name} - PyPDF2 nÃ£o disponÃ­vel"
            except Exception as e:
                return f"Erro ao processar PDF {file_path.name}: {str(e)}"
        
        # Imagens
        elif file_ext in ['.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff', '.webp']:
            return f"Arquivo de imagem {file_path.name} - OCR nÃ£o implementado ainda"
        
        # Documentos Word
        elif file_ext in ['.docx', '.doc']:
            try:
                import docx
                doc = docx.Document(file_path)
                text = ""
                for paragraph in doc.paragraphs:
                    text += paragraph.text + "\n"
                return text
            except ImportError:
                return f"Arquivo Word {file_path.name} - python-docx nÃ£o disponÃ­vel"
            except Exception as e:
                return f"Erro ao processar Word {file_path.name}: {str(e)}"
        
        # PowerPoint
        elif file_ext in ['.pptx', '.ppt']:
            return f"Arquivo PowerPoint {file_path.name} - Processamento nÃ£o implementado ainda"
        
        # ODF
        elif file_ext in ['.odt', '.ods', '.odp']:
            return f"Arquivo ODF {file_path.name} - Processamento nÃ£o implementado ainda"
        
        # Fallback para outros tipos
        else:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return f.read()
            except:
                return f"Arquivo {file_path.name} - Tipo nÃ£o suportado para extraÃ§Ã£o de texto"
    
    except Exception as e:
        return f"Erro ao processar {file_path.name}: {str(e)}"

def process_document(file_path: Path) -> dict:
    """Processar documento"""
    try:
        with st.spinner(f"ğŸ“– Processando {file_path.name}..."):
            # Extrair conteÃºdo baseado no tipo de arquivo
            content = extract_content_from_file(file_path)
            
            if not content.strip():
                return {'success': False, 'error': 'Nenhum conteÃºdo extraÃ­do'}
            
            # Extrair URLs
            url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
            urls = re.findall(url_pattern, content)
            
            # Fazer scraping de URLs
            scraped_content = ""
            if urls:
                for url in urls[:2]:  # Limitar a 2 URLs
                    try:
                        req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})
                        with urllib.request.urlopen(req, timeout=3) as response:
                            html_content = response.read().decode('utf-8', errors='ignore')
                            # Extrair texto do HTML
                            class HTMLTextExtractor(HTMLParser):
                                def __init__(self):
                                    super().__init__()
                                    self.text = []
                                
                                def handle_data(self, data):
                                    self.text.append(data)
                                
                                def get_text(self):
                                    return ' '.join(self.text)
                            
                            extractor = HTMLTextExtractor()
                            extractor.feed(html_content)
                            scraped_text = extractor.get_text()
                            scraped_content += f"\n\n--- ConteÃºdo de {url} ---\n{scraped_text}"
                    except:
                        pass
            
            # Combinar conteÃºdo
            full_content = content + scraped_content
            
            # Criar ID Ãºnico
            doc_id = hashlib.md5(str(file_path).encode()).hexdigest()[:16]
            
            # Salvar no banco com timeout
            conn = sqlite3.connect(st.session_state['vector_db_path'], timeout=30.0)
            cursor = conn.cursor()
            
            # Configurar para WAL mode para evitar locks
            cursor.execute("PRAGMA journal_mode=WAL")
            cursor.execute("PRAGMA synchronous=NORMAL")
            
            metadata = {
                'file_path': str(file_path),
                'file_type': file_path.suffix,
                'word_count': len(full_content.split()),
                'char_count': len(full_content),
                'urls_found': len(urls),
                'processed_at': datetime.now().isoformat()
            }
            
            cursor.execute('''
                INSERT OR REPLACE INTO documents (id, file_path, file_type, content, metadata)
                VALUES (?, ?, ?, ?, ?)
            ''', (doc_id, str(file_path), file_path.suffix, full_content, json.dumps(metadata)))
            
            # Criar chunks
            chunk_size = 1000
            chunks = [full_content[i:i+chunk_size] for i in range(0, len(full_content), chunk_size)]
            
            # Salvar chunks com embeddings
            rag_system = RAGSystem(st.session_state['vector_db_path'])
            
            for i, chunk in enumerate(chunks):
                chunk_id = f"{doc_id}_chunk_{i}"
                embedding = rag_system.create_embedding(chunk)
                
                chunk_metadata = {
                    'chunk_index': i,
                    'total_chunks': len(chunks),
                    'chunk_size': len(chunk)
                }
                
                cursor.execute('''
                    INSERT OR REPLACE INTO chunks (id, document_id, chunk_text, chunk_index, vector, metadata)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (chunk_id, doc_id, chunk, i, json.dumps(embedding), json.dumps(chunk_metadata)))
            
            conn.commit()
            conn.close()
            
            return {
                'success': True,
                'doc_id': doc_id,
                'text_length': len(full_content),
                'urls_count': len(urls),
                'chunks_count': len(chunks)
            }
            
    except Exception as e:
        return {'success': False, 'error': str(e)}

def process_selected_files():
    """Processar arquivos selecionados"""
    try:
        status = st.session_state['processing_status']
        selected_files = st.session_state['selected_files']
        
        if not selected_files:
            st.error("âŒ Nenhum arquivo selecionado!")
            return
        
        # Resetar status
        status['is_processing'] = True
        status['processed_files'] = []
        status['failed_files'] = []
        status['logs'] = []
        status['start_time'] = datetime.now()
        status['total_files'] = len(selected_files)
        
        # Log de inÃ­cio
        log_entry = {
            'timestamp': datetime.now().strftime('%H:%M:%S'),
            'level': 'INFO',
            'message': f"ğŸš€ Iniciando processamento de {len(selected_files)} arquivos selecionados",
            'module': 'interface_corrigida'
        }
        status['logs'].append(log_entry)
        
        # Barra de progresso
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # Processar arquivos sequencialmente para evitar locks
        for i, file_path in enumerate(selected_files):
            status['current_file'] = str(file_path)
            status['progress'] = (i / len(selected_files)) * 100
            
            # Atualizar progresso
            progress_bar.progress(status['progress'] / 100)
            status_text.text(f"ğŸ”„ Processando: {file_path.name}")
            
            # Processar arquivo
            result = process_document(file_path)
            
            # Pequeno delay para evitar locks
            time.sleep(0.5)
            
            if result['success']:
                status['processed_files'].append({
                    'file': str(file_path),
                    'size': result['text_length'],
                    'urls': result['urls_count'],
                    'chunks': result['chunks_count'],
                    'timestamp': datetime.now().strftime('%H:%M:%S')
                })
                log_entry = {
                    'timestamp': datetime.now().strftime('%H:%M:%S'),
                    'level': 'SUCCESS',
                    'message': f"âœ… {file_path.name} ({result['text_length']} chars, {result['urls_count']} URLs, {result['chunks_count']} chunks)",
                    'module': 'interface_corrigida'
                }
            else:
                status['failed_files'].append({
                    'file': str(file_path),
                    'error': result['error'],
                    'timestamp': datetime.now().strftime('%H:%M:%S')
                })
                log_entry = {
                    'timestamp': datetime.now().strftime('%H:%M:%S'),
                    'level': 'WARNING',
                    'message': f"âš ï¸ {file_path.name} - {result['error']}",
                    'module': 'interface_corrigida'
                }
            
            status['logs'].append(log_entry)
            
            # Manter apenas os Ãºltimos 50 logs
            if len(status['logs']) > 50:
                status['logs'] = status['logs'][-50:]
        
        # Finalizar
        status['is_processing'] = False
        status['current_file'] = None
        status['progress'] = 100
        
        # ConclusÃ£o
        progress_bar.progress(1.0)
        status_text.text("ğŸ‰ Processamento concluÃ­do!")
        
        log_entry = {
            'timestamp': datetime.now().strftime('%H:%M:%S'),
            'level': 'INFO',
            'message': f"ğŸ‰ Processamento concluÃ­do! {len(status['processed_files'])} processados, {len(status['failed_files'])} falharam",
            'module': 'interface_corrigida'
        }
        status['logs'].append(log_entry)
        
        # Limpar elementos
        progress_bar.empty()
        status_text.empty()
        
    except Exception as e:
        status = st.session_state['processing_status']
        status['is_processing'] = False
        log_entry = {
            'timestamp': datetime.now().strftime('%H:%M:%S'),
            'level': 'ERROR',
            'message': f"Erro geral: {str(e)}",
            'module': 'interface_corrigida'
        }
        status['logs'].append(log_entry)

# ============================================================================
# INTERFACE PRINCIPAL CORRIGIDA
# ============================================================================

def main():
    """Interface principal corrigida"""
    
    # TÃ­tulo
    st.title("ğŸ¤– Sistema RAG Local - Interface Corrigida")
    st.markdown("**Sistema RAG 100% Open Source com ConfiguraÃ§Ã£o e Resultados Ãšteis**")
    
    # Inicializar sistema RAG
    if st.session_state['rag_system'] is None:
        with st.spinner("ğŸ§  Inicializando sistema RAG..."):
            st.session_state['rag_system'] = RAGSystem(st.session_state['vector_db_path'])
    
    # Sidebar com configuraÃ§Ã£o
    with st.sidebar:
        st.header("ğŸ›ï¸ ConfiguraÃ§Ã£o")
        
        # ConfiguraÃ§Ã£o do diretÃ³rio
        st.subheader("ğŸ“ DiretÃ³rio de Documentos")
        current_dir = st.text_input(
            "DiretÃ³rio dos documentos:", 
            value=st.session_state['documents_dir'],
            help="Caminho para o diretÃ³rio com os arquivos a serem processados"
        )
        
        if st.button("ğŸ”„ Atualizar DiretÃ³rio"):
            st.session_state['documents_dir'] = current_dir
            st.success(f"DiretÃ³rio atualizado para: {current_dir}")
            st.rerun()
        
        # Listar arquivos disponÃ­veis
        st.subheader("ğŸ“„ Arquivos DisponÃ­veis")
        available_files = get_available_files(st.session_state['documents_dir'])
        
        if available_files:
            # Agrupar por tipo
            file_types = {}
            for file_path in available_files:
                ext = file_path.suffix.lower()
                if ext not in file_types:
                    file_types[ext] = []
                file_types[ext].append(file_path)
            
            # Mostrar estatÃ­sticas por tipo
            st.info(f"ğŸ“Š Encontrados {len(available_files)} arquivos no diretÃ³rio")
            
            # Mostrar tipos encontrados
            col1, col2, col3 = st.columns(3)
            with col1:
                st.write("**Tipos encontrados:**")
                for ext, files in sorted(file_types.items()):
                    st.write(f"â€¢ {ext}: {len(files)} arquivos")
            
            # SeleÃ§Ã£o de arquivos
            file_options = [f.name for f in available_files]
            selected_file_names = st.multiselect(
                "Selecionar arquivos para processar:",
                file_options,
                default=file_options[:10] if len(file_options) > 10 else file_options,
                help="Selecione os arquivos que deseja processar"
            )
            
            # Atualizar arquivos selecionados
            if selected_file_names:
                st.session_state['selected_files'] = [
                    f for f in available_files if f.name in selected_file_names
                ]
                st.success(f"âœ… {len(st.session_state['selected_files'])} arquivos selecionados")
            else:
                st.session_state['selected_files'] = []
                st.warning("âš ï¸ Nenhum arquivo selecionado")
        else:
            st.error(f"âŒ Nenhum arquivo encontrado em: {st.session_state['documents_dir']}")
            st.session_state['selected_files'] = []
        
        # BotÃ£o de processamento
        st.subheader("ğŸš€ Processamento")
        if st.button("ğŸš€ PROCESSAR ARQUIVOS SELECIONADOS", type="primary", use_container_width=True):
            if not st.session_state['processing_status']['is_processing']:
                process_selected_files()
                st.rerun()
        
        # BotÃ£o para consulta RAG
        st.subheader("ğŸ” Consulta RAG")
        question = st.text_area("Digite sua pergunta:", height=100, placeholder="Ex: O que Ã© machine learning? Como funciona deep learning?")
        
        if st.button("ğŸ” FAZER CONSULTA", type="secondary", use_container_width=True):
            if question.strip():
                with st.spinner("ğŸ” Buscando respostas..."):
                    rag_system = st.session_state['rag_system']
                    results = rag_system.query(question)
                
                if results:
                    st.success(f"âœ… Encontradas {len(results)} respostas")
                    
                    # Mostrar resultados melhorados
                    for i, result in enumerate(results[:3], 1):
                        with st.expander(f"Resposta {i} - Similaridade: {result['similarity']:.3f}"):
                            st.write(f"**Arquivo:** {Path(result['file_path']).name}")
                            st.write(f"**Tipo:** {result['file_type']}")
                            st.write(f"**Vetorial:** {result['vector_sim']:.3f} | **Textual:** {result['text_sim']:.3f}")
                            st.write(f"**ConteÃºdo:**")
                            st.text(result['chunk_text'][:800])
                    
                    # Gerar e salvar resumo
                    with st.spinner("ğŸ“ Gerando resumo..."):
                        try:
                            summary_path = rag_system.save_summary(question, results)
                            st.success(f"ğŸ“„ Resumo salvo em: {summary_path}")
                            
                            # Mostrar resumo
                            with open(summary_path, 'r', encoding='utf-8') as f:
                                summary_content = f.read()
                            st.markdown("**Resumo gerado:**")
                            st.markdown(summary_content)
                        except Exception as e:
                            st.error(f"Erro ao gerar resumo: {str(e)}")
                            # Mostrar resumo bÃ¡sico mesmo com erro
                            summary_text = rag_system.generate_summary(question, results)
                            st.markdown("**Resumo bÃ¡sico:**")
                            st.markdown(summary_text)
                else:
                    st.warning("âŒ Nenhuma resposta relevante encontrada")
        
        # EstatÃ­sticas
        st.subheader("ğŸ“Š EstatÃ­sticas")
        stats = st.session_state['rag_system'].get_stats()
        
        st.metric("ğŸ“„ Documentos", stats['documents'])
        st.metric("ğŸ“ Chunks", stats['chunks'])
        
        # Status do processamento
        status = st.session_state['processing_status']
        if status['is_processing']:
            st.info(f"ğŸ”„ Processando: {Path(status['current_file']).name if status['current_file'] else 'N/A'}")
            st.progress(status['progress'] / 100)
        elif status['processed_files']:
            st.success(f"âœ… {len(status['processed_files'])} arquivos processados")
        else:
            st.warning("â¸ï¸ Nenhum processamento em andamento")
    
    # ConteÃºdo principal
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.header("ğŸ“Š Dashboard")
        
        # MÃ©tricas principais
        stats = st.session_state['rag_system'].get_stats()
        
        col1_1, col1_2, col1_3 = st.columns(3)
        with col1_1:
            st.metric("ğŸ“„ Documentos", stats['documents'])
        with col1_2:
            st.metric("ğŸ“ Chunks", stats['chunks'])
        with col1_3:
            st.metric("ğŸ¤– Sistema", "Ativo")
        
        # Status do processamento
        status = st.session_state['processing_status']
        if status['is_processing']:
            st.info(f"ğŸ”„ **Processando:** {Path(status['current_file']).name if status['current_file'] else 'N/A'}")
            st.progress(status['progress'] / 100)
        elif status['processed_files']:
            st.success(f"âœ… **Processamento concluÃ­do:** {len(status['processed_files'])} arquivos processados")
        else:
            st.warning("â¸ï¸ **Nenhum processamento em andamento**")
        
        # Lista de arquivos processados
        if status['processed_files']:
            st.subheader("ğŸ“ Arquivos Processados (Ãšltimos 10)")
            for file_info in status['processed_files'][-10:]:
                st.write(f"ğŸ“„ **{Path(file_info['file']).name}** - {file_info['size']} chars, {file_info['urls']} URLs, {file_info['chunks']} chunks - {file_info['timestamp']}")
        
        # Arquivos com falha
        if status['failed_files']:
            st.subheader("âŒ Arquivos com Falha (Ãšltimos 5)")
            for file_info in status['failed_files'][-5:]:
                st.write(f"âŒ **{Path(file_info['file']).name}** - {file_info['error']} - {file_info['timestamp']}")
        
        # InformaÃ§Ãµes do banco de dados
        st.subheader("ğŸ’¾ Banco de Dados")
        st.info(f"**LocalizaÃ§Ã£o:** `{st.session_state['vector_db_path']}`")
        st.info(f"**DiretÃ³rio processado:** `{st.session_state['documents_dir']}`")
        
        if stats['file_info']:
            st.write("**Arquivos no banco:**")
            for file_path, file_type, count in stats['file_info'][:10]:
                st.write(f"â€¢ {Path(file_path).name} ({file_type}) - {count} registros")
        
        # Resumos gerados
        st.subheader("ğŸ“„ Resumos Gerados")
        summaries_dir = Path("summaries")
        if summaries_dir.exists():
            summary_files = list(summaries_dir.glob("*.md"))
            if summary_files:
                st.info(f"ğŸ“Š {len(summary_files)} resumos encontrados")
                for summary_file in sorted(summary_files, key=lambda x: x.stat().st_mtime, reverse=True)[:5]:
                    st.write(f"ğŸ“„ **{summary_file.name}**")
                    st.write(f"   Criado: {datetime.fromtimestamp(summary_file.stat().st_mtime).strftime('%d/%m/%Y %H:%M:%S')}")
                    st.write(f"   Tamanho: {summary_file.stat().st_size} bytes")
            else:
                st.info("Nenhum resumo gerado ainda")
        else:
            st.info("DiretÃ³rio de resumos nÃ£o existe ainda")
    
    with col2:
        st.header("ğŸ“ Logs")
        
        # Filtros
        log_level = st.selectbox("Filtrar logs:", ["TODOS", "INFO", "SUCCESS", "WARNING", "ERROR"])
        
        # Logs
        status = st.session_state['processing_status']
        if status['logs']:
            # Filtrar logs
            filtered_logs = status['logs']
            if log_level != "TODOS":
                filtered_logs = [log for log in status['logs'] if log['level'] == log_level]
            
            # Mostrar logs
            for log in filtered_logs[-15:]:  # Ãšltimos 15
                level_emoji = {
                    'INFO': 'â„¹ï¸',
                    'SUCCESS': 'âœ…',
                    'WARNING': 'âš ï¸',
                    'ERROR': 'âŒ'
                }.get(log['level'], 'ğŸ“')
                
                st.write(f"{level_emoji} **{log['timestamp']}** {log['message']}")
        else:
            st.info("Nenhum log disponÃ­vel ainda")
        
        # Auto-refresh
        if status['is_processing']:
            time.sleep(2)
            st.rerun()
    
    # InformaÃ§Ãµes do sistema
    st.markdown("---")
    st.subheader("ğŸ¤– Sistema RAG")
    
    col3, col4, col5 = st.columns(3)
    
    with col3:
        st.write("**ğŸ¤– Sistema:**")
        st.write("â€¢ ConfiguraÃ§Ã£o de diretÃ³rio")
        st.write("â€¢ SeleÃ§Ã£o de arquivos")
        st.write("â€¢ Processamento controlado")
        st.write("â€¢ Resultados Ãºteis")
    
    with col4:
        st.write("**ğŸ“ Processamento:**")
        st.write(f"â€¢ DiretÃ³rio: `{st.session_state['documents_dir']}`")
        st.write(f"â€¢ Banco: `{st.session_state['vector_db_path']}`")
        st.write("â€¢ ExtensÃµes: .txt, .md, .py, .js, .html, .css, .json, .yaml")
        st.write("â€¢ URL scraping automÃ¡tico")
    
    with col5:
        st.write("**âš¡ Status:**")
        if status['is_processing']:
            st.write("ğŸ”„ Processando...")
        elif status['processed_files']:
            st.write("âœ… Sistema pronto")
        else:
            st.write("â¸ï¸ Aguardando processamento")
    
    # Footer
    st.markdown("---")
    st.markdown("**ğŸ¤– Sistema RAG Local - Interface Corrigida - 100% Open Source - Desenvolvido com â¤ï¸**")

if __name__ == "__main__":
    main()
